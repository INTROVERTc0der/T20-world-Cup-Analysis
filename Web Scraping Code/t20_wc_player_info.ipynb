{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "players_info =[]"
      ],
      "metadata": {
        "id": "Zslshy-XGIhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_player_data(soup):\n",
        "    # Extracting player details\n",
        "    paras = soup.find_all('p', class_='ds-text-tight-s ds-font-regular ds-uppercase ds-text-typo-mid3')\n",
        "    batting_style = None\n",
        "    bowling_style = None\n",
        "    playing_role = None\n",
        "    for para in paras:\n",
        "        if para.text == 'Batting Style':\n",
        "            batting_style = para.find_next('span').text\n",
        "        elif para.text == 'Bowling Style':\n",
        "            bowling_style = para.find_next('span').text\n",
        "        elif para.text == 'Playing Role':\n",
        "            playing_role = para.find_next('span').text\n",
        "\n",
        "\n",
        "    # Extracting description\n",
        "\n",
        "    description = soup.select_one('div',class_='ci-player-bio-content').text\n",
        "\n",
        "    return {\n",
        "        \"battingStyle\": batting_style,\n",
        "        \"bowlingStyle\": bowling_style,\n",
        "        \"playingRole\": playing_role,\n",
        "        \"content\": description\n",
        "    }\n"
      ],
      "metadata": {
        "id": "k8sHohW3-qZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_stage_stage3(name, team, url):\n",
        "    # Step 1: Navigate to the match URL\n",
        "    headers = {\n",
        "      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Proceed with parsing the HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Step 2: Parse player details\n",
        "        final_data = parse_player_data(soup)\n",
        "\n",
        "        # Step 3: Collect and store final player data\n",
        "        player_info = {\n",
        "            \"name\": name,\n",
        "            \"team\": team,\n",
        "            \"battingStyle\": final_data['battingStyle'],\n",
        "            \"bowlingStyle\": final_data['bowlingStyle'],\n",
        "            \"playingRole\": final_data['playingRole'],\n",
        "            \"description\": final_data['content'],\n",
        "        }\n",
        "\n",
        "        # Here, you would typically store or process the player_info data\n",
        "        players_info.append(player_info)\n",
        "    else:\n",
        "        print(\"Failed to retrieve data from the website. Status code:\", response.status_code)\n",
        "\n"
      ],
      "metadata": {
        "id": "MafYcPfY-EfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def next_stage_stage2(url):\n",
        "    # Step 1: Navigate to the match URL\n",
        "    headers = {\n",
        "      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Proceed with parsing the HTML content\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Step 2: Parse player data\n",
        "        players_data = []\n",
        "\n",
        "        # Identifying teams\n",
        "        match_details = soup.find_all('div',class_='ds-bg-fill-canvas')\n",
        "        # print(match_details)\n",
        "\n",
        "        teams= match_details[0].find_all('span',class_='ds-text-tight-xs')\n",
        "        team1 = teams[0].text.replace(\" Innings\",\"\")\n",
        "        team2 = teams[1].text.replace(\" Innings\",\"\")\n",
        "        # Extracting batting players\n",
        "        table_div= soup.find_all('div',class_ = 'ds-p-0')\n",
        "        first_innings_rows = (table_div[1].find_all('table'))[0].select('tbody > tr')\n",
        "        second_innings_rows = (table_div[2].find_all('table'))[0].select('tbody > tr')\n",
        "\n",
        "\n",
        "        for row in first_innings_rows:\n",
        "            if len(row.find_all(\"td\")) >= 8:\n",
        "                outerSpan = (row.select('td')[0].select('td span.ds-text-tight-s.ds-font-medium.ds-text-typo.ds-underline.ds-decoration-ui-stroke.hover\\\\:ds-text-typo-primary.hover\\\\:ds-decoration-ui-stroke-primary.ds-block'))[0]\n",
        "                name = outerSpan.find_all('span')[0].text.strip()\n",
        "                link = \"https://www.espncricinfo.com\" + row.select('td')[0].find('a')['href']\n",
        "                players_data.append({\"name\": name, \"team\": team1, \"link\": link})\n",
        "\n",
        "        for row in second_innings_rows:\n",
        "            if len(row.find_all(\"td\")) >= 8:\n",
        "                outerSpan = (row.select('td')[0].select('td span.ds-text-tight-s.ds-font-medium.ds-text-typo.ds-underline.ds-decoration-ui-stroke.hover\\\\:ds-text-typo-primary.hover\\\\:ds-decoration-ui-stroke-primary.ds-block'))[0]\n",
        "                name = outerSpan.find_all('span')[0].text.strip()\n",
        "                link = \"https://www.espncricinfo.com\" + row.select('td')[0].find('a')['href']\n",
        "                players_data.append({\"name\": name, \"team\": team2, \"link\": link})\n",
        "\n",
        "        # Extracting bowling players\n",
        "\n",
        "        first_innings_bowlers = (table_div[1].find_all('table'))[1].select('tbody > tr')\n",
        "        second_innings_bowlers = (table_div[2].find_all('table'))[1].select('tbody > tr')\n",
        "\n",
        "        for row in first_innings_bowlers:\n",
        "            if len(row.find_all(\"td\")) >= 11:\n",
        "                name = (row.select('td')[0].select('span.ds-text-tight-s.ds-font-medium.ds-text-typo.ds-underline.ds-decoration-ui-stroke.hover\\\\:ds-text-typo-primary.hover\\\\:ds-decoration-ui-stroke-primary.ds-block'))[0].text.strip()\n",
        "                link = \"https://www.espncricinfo.com\" + row.select('td')[0].find('a')['href']\n",
        "                players_data.append({\"name\": name, \"team\": team2, \"link\": link})\n",
        "\n",
        "        for row in second_innings_bowlers:\n",
        "            if len(row.find_all(\"td\")) >= 11:\n",
        "                name = (row.select('td')[0].select('span.ds-text-tight-s.ds-font-medium.ds-text-typo.ds-underline.ds-decoration-ui-stroke.hover\\\\:ds-text-typo-primary.hover\\\\:ds-decoration-ui-stroke-primary.ds-block'))[0].text.strip()\n",
        "                link = \"https://www.espncricinfo.com\" + row.select('td')[0].find('a')['href']\n",
        "                players_data.append({\"name\": name, \"team\": team1, \"link\": link})\n",
        "\n",
        "        # Moving to the next stage with each player data\n",
        "        for player in players_data:\n",
        "            next_stage_stage3(name=player['name'], team=player['team'], url=player['link'])\n",
        "    else:\n",
        "        print(\"Failed to retrieve data from the website. Status code:\", response.status_code)\n"
      ],
      "metadata": {
        "id": "R2SlD1HUyfGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lw2X89XbxZup"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Step 1: Navigate to the webpage\n",
        "url = 'https://stats.espncricinfo.com/ci/engine/records/team/match_results.html?id=14450;type=tournament'\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "}\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Proceed with parsing the HTML content\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Step 2: Parse match summary links\n",
        "    match_summary_links = []\n",
        "    table_body = soup.select('#main-container > div.ds-relative > div > div.ds-flex.ds-space-x-5 > div.ds-grow > div:nth-child(2) > div > div:nth-child(1) > div.ds-overflow-x-auto.ds-scrollbar-hide > table > tbody')[0]\n",
        "    table_rows = table_body.find_all('tr')\n",
        "\n",
        "    for row in table_rows:\n",
        "        tds = row.find_all('td')\n",
        "        row_url = \"https://www.espncricinfo.com\" + tds[6].find('a')['href']\n",
        "        match_summary_links.append(row_url)\n",
        "\n",
        "    # Moving to the next stage with each match URL\n",
        "    for link in match_summary_links:\n",
        "        next_stage_stage2(url=link)\n",
        "else:\n",
        "    print(\"Failed to retrieve data from the website. Status code:\", response.status_code)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Specify the file path where you want to store the JSON file\n",
        "file_path = 't20_wc_player_info.json'\n",
        "\n",
        "# Writing the list to a JSON file\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(players_info, json_file, indent=4)\n",
        "\n",
        "print(f\"List has been stored in {file_path}\")\n"
      ],
      "metadata": {
        "id": "tth0Q9IcBI-y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "288f07ee-3b55-4df2-e083-3265e1491fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List has been stored in t20_wc_player_info.json\n"
          ]
        }
      ]
    }
  ]
}